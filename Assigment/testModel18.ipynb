{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]: Imports and global settings\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "# Grid & agent parameters\n",
    "GRID_SIZE   = 5\n",
    "NUM_AGENTS  = 4\n",
    "A_POS       = (0, 0)        # Pickup cell\n",
    "B_POS       = (4, 4)        # Drop-off cell\n",
    "ACTIONS     = [(-1,0),(1,0),(0,-1),(0,1)]  # N, S, W, E\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "# Q-learning hyperparameters\n",
    "ALPHA   = 0.1   # learning rate\n",
    "GAMMA   = 0.99  # discount factor\n",
    "EPSILON = 0.2   # exploration rate\n",
    "\n",
    "# Training budgets (step, collision, time)\n",
    "STEP_BUDGET   = 1_500_000\n",
    "COLLISION_BUDGET = 4_000\n",
    "WALLTIME_BUDGET = 600    # seconds\n",
    "\n",
    "# Performance test parameters\n",
    "TEST_EPISODES     = 1000\n",
    "TEST_STEP_LIMIT   = 20\n",
    "FINAL_SUCCESS_RATE = 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]: Updated Environment with off-the-job flag\n",
    "\n",
    "class MultiAgentEnv:\n",
    "    def __init__(self, off_job_training: bool = True):\n",
    "        \"\"\"\n",
    "        off_job_training=True:\n",
    "          - fixed start: 2 agents at A, 2 at B each episode\n",
    "        off_job_training=False:\n",
    "          - random start: all agents and A/B randomised each episode\n",
    "        \"\"\"\n",
    "        self.size      = GRID_SIZE\n",
    "        self.num_agents = NUM_AGENTS\n",
    "        self.off_job   = off_job_training\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        if self.off_job:\n",
    "            # ---- Off‐the‐job training: fixed 2 at A, 2 at B ----\n",
    "            self.agent_pos = [A_POS]*2 + [B_POS]*2\n",
    "            self.carrying   = [False]*2 + [True]*2\n",
    "        else:\n",
    "            # ---- Standard: randomise agent starts and initial carrying states ----\n",
    "            self.agent_pos = []\n",
    "            self.carrying  = []\n",
    "            for _ in range(self.num_agents):\n",
    "                # random between A or B\n",
    "                if random.random() < 0.5:\n",
    "                    self.agent_pos.append(A_POS)\n",
    "                    self.carrying.append(False)\n",
    "                else:\n",
    "                    self.agent_pos.append(B_POS)\n",
    "                    self.carrying.append(True)\n",
    "\n",
    "        self.steps      = 0\n",
    "        self.collisions = 0\n",
    "        return self._get_observations()\n",
    "\n",
    "    def _get_observations(self):\n",
    "        \"\"\"\n",
    "        Opposite‐direction sensor ONLY:\n",
    "        for each of the 8 neighbours, set 1 if occupied by an agent\n",
    "        whose direction (A→B vs B→A) is opposite to self.\n",
    "        \"\"\"\n",
    "        obs = []\n",
    "        for i in range(self.num_agents):\n",
    "            x, y    = self.agent_pos[i]\n",
    "            dir_i   = 'A2B' if not self.carrying[i] else 'B2A'\n",
    "            opp_mask = []\n",
    "            for dx in (-1, 0, 1):\n",
    "                for dy in (-1, 0, 1):\n",
    "                    if dx == 0 and dy == 0:\n",
    "                        continue\n",
    "                    nx, ny = x+dx, y+dy\n",
    "                    bit = 0\n",
    "                    if 0 <= nx < self.size and 0 <= ny < self.size:\n",
    "                        for j in range(self.num_agents):\n",
    "                            if (nx, ny) == self.agent_pos[j]:\n",
    "                                dir_j = 'A2B' if not self.carrying[j] else 'B2A'\n",
    "                                if dir_j != dir_i:\n",
    "                                    bit = 1\n",
    "                                break\n",
    "                    opp_mask.append(bit)\n",
    "            obs.append((x, y, int(self.carrying[i]), tuple(opp_mask)))\n",
    "        return obs\n",
    "\n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Central clock update in fixed order 0→1→…→N−1.\n",
    "        Count only head‐on collisions.\n",
    "        \"\"\"\n",
    "        self.steps += 1\n",
    "        prev = list(self.agent_pos)\n",
    "        for i, a in enumerate(actions):\n",
    "            dx, dy = ACTIONS[a]\n",
    "            x, y   = self.agent_pos[i]\n",
    "            nx, ny = x+dx, y+dy\n",
    "            # wall check\n",
    "            if 0 <= nx < self.size and 0 <= ny < self.size:\n",
    "                self.agent_pos[i] = (nx, ny)\n",
    "            # detect head-on with any earlier-updated agent\n",
    "            for j in range(i):\n",
    "                if (self.agent_pos[i] == prev[j] and\n",
    "                    self.agent_pos[j] == prev[i]):\n",
    "                    self.collisions += 1\n",
    "            # auto pickup/dropoff\n",
    "            if not self.carrying[i] and self.agent_pos[i] == A_POS:\n",
    "                self.carrying[i] = True\n",
    "            elif self.carrying[i] and self.agent_pos[i] == B_POS:\n",
    "                self.carrying[i] = False\n",
    "\n",
    "        return self._get_observations(), self.collisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]: Independent Tabular Q-learning agents\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self):\n",
    "        # Q-table mapping state→action values\n",
    "        self.Q = defaultdict(lambda: np.zeros(NUM_ACTIONS))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # epsilon-greedy\n",
    "        if random.random() < EPSILON:\n",
    "            return random.randrange(NUM_ACTIONS)\n",
    "        qvals = self.Q[state]\n",
    "        return int(np.argmax(qvals))\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        q = self.Q[state][action]\n",
    "        q_next = np.max(self.Q[next_state])\n",
    "        # Q-learning update\n",
    "        self.Q[state][action] = q + ALPHA*(reward + GAMMA*q_next - q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished in 308540 steps, 4000 collisions, time elapsed 4 s\n"
     ]
    }
   ],
   "source": [
    "# In[4]: Non-episodic Q-learning training loop \n",
    "import time\n",
    "# 1) Test with just Central clock + Opposite-direction sensor:\n",
    "# 2) Then, if you want to add Off-the-job training as well:\n",
    "# env = MultiAgentEnv(off_job_training=True)\n",
    "# In[4]: Training loop (fixed)\n",
    "\n",
    "# Instantiate with or without off-the-job as you like:\n",
    "env = MultiAgentEnv(off_job_training=False)\n",
    "\n",
    "# Create one Q-learning agent per grid agent\n",
    "agents = [QAgent() for _ in range(NUM_AGENTS)]\n",
    "\n",
    "# Initialize counters and get initial (non-episodic) state\n",
    "total_steps      = 0\n",
    "total_collisions = 0\n",
    "start_time       = time.time()\n",
    "states           = env.reset()\n",
    "\n",
    "# Continue learning until any budget is hit\n",
    "while (total_steps < STEP_BUDGET\n",
    "       and total_collisions < COLLISION_BUDGET\n",
    "       and time.time() - start_time < WALLTIME_BUDGET):\n",
    "\n",
    "    # 1) Central‐clock action selection\n",
    "    actions = [agents[i].select_action(states[i])\n",
    "               for i in range(NUM_AGENTS)]\n",
    "\n",
    "    # 2) Environment step: returns next states and cumulative collisions\n",
    "    prev_collisions = total_collisions\n",
    "    next_states, collisions = env.step(actions)\n",
    "\n",
    "    # 3) Q-learning updates for each agent\n",
    "    for i in range(NUM_AGENTS):\n",
    "        reward = -1                       # step penalty\n",
    "\n",
    "        # collision penalty if any new head-on collision occurred\n",
    "        if collisions > prev_collisions:\n",
    "            reward -= 10\n",
    "\n",
    "        # successful drop-off detection (A→B→A cycle)\n",
    "        # we know a drop-off just happened if carrying→not carrying\n",
    "        if states[i][2] == 1 and next_states[i][2] == 0:\n",
    "            reward += 10\n",
    "\n",
    "        # update Q-table for agent i\n",
    "        agents[i].update(states[i], actions[i], reward, next_states[i])\n",
    "\n",
    "    # 4) Advance for next loop iteration\n",
    "    states = next_states\n",
    "    total_steps      += NUM_AGENTS\n",
    "    total_collisions  = collisions\n",
    "\n",
    "# summary\n",
    "print(f\"Training finished in {total_steps} steps, \"\n",
    "      f\"{total_collisions} collisions, \"\n",
    "      f\"time elapsed {int(time.time()-start_time)} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 246948 steps, 4000 collisions\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "ALPHA             = 0.01    # lower LR\n",
    "GAMMA             = 0.99\n",
    "COLLISION_PENALTY = 100     # heavy penalty\n",
    "EPS_START, EPS_END = 1.0, 0.01\n",
    "TOTAL_AGENT_ACTIONS = STEP_BUDGET  # 1.5M\n",
    "eps_decay         = (EPS_START - EPS_END)/(TOTAL_AGENT_ACTIONS/2)\n",
    "\n",
    "# instantiate\n",
    "env     = MultiAgentEnv(off_job_training=True)\n",
    "agents  = [QAgent() for _ in range(NUM_AGENTS)]\n",
    "states  = env.reset()\n",
    "epsilon = EPS_START\n",
    "steps   = 0\n",
    "colls   = 0\n",
    "start   = time.time()\n",
    "\n",
    "while (steps < STEP_BUDGET \n",
    "       and colls < COLLISION_BUDGET \n",
    "       and time.time()-start < WALLTIME_BUDGET):\n",
    "\n",
    "    prev_colls = env.collisions\n",
    "    actions    = []\n",
    "    # select with linear decay epsilon\n",
    "    for i in range(NUM_AGENTS):\n",
    "        if random.random() < epsilon:\n",
    "            actions.append(random.randrange(NUM_ACTIONS))\n",
    "        else:\n",
    "            actions.append(int(np.argmax(agents[i].Q[states[i]])))\n",
    "\n",
    "    # decay eps\n",
    "    epsilon = max(EPS_END, epsilon - eps_decay)\n",
    "\n",
    "    next_states, _ = env.step(actions)\n",
    "    new_colls      = env.collisions\n",
    "    n_coll         = new_colls - prev_colls\n",
    "\n",
    "    # Q-updates\n",
    "    for i in range(NUM_AGENTS):\n",
    "        r = -1  # step cost\n",
    "\n",
    "        # neighbourhood anticipation penalty\n",
    "        # unpack the last 8 bits of the state\n",
    "        neigh_bits = states[i][3]\n",
    "        if any(neigh_bits):\n",
    "            r -= 2\n",
    "\n",
    "        # collision penalty only if this agent actually collided?\n",
    "        # (for now we apply to all if any head-on happened)\n",
    "        if n_coll > 0:\n",
    "            r -= COLLISION_PENALTY * n_coll\n",
    "\n",
    "        # successful dropoff\n",
    "        if states[i][2]==1 and next_states[i][2]==0:\n",
    "            r += 10\n",
    "\n",
    "        agents[i].update(states[i], actions[i], r, next_states[i])\n",
    "\n",
    "    states = next_states\n",
    "    steps  += NUM_AGENTS\n",
    "    colls   = new_colls\n",
    "\n",
    "print(f\"Done: {steps} steps, {colls} collisions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test success rate (≤20 steps): 48.4%\n",
      "Head-on collisions during training: 16443\n",
      "Performance points earned: 0 / 2 per metric, 4 max\n"
     ]
    }
   ],
   "source": [
    "# In[ ]: Performance evaluation (Table 2)\n",
    "\n",
    "# 1) Test success rate (≤20 steps, no collisions)\n",
    "test_env = MultiAgentEnv(off_job_training=False)\n",
    "successes = 0\n",
    "\n",
    "for _ in range(TEST_EPISODES):\n",
    "    states   = test_env.reset()\n",
    "    collided = False\n",
    "\n",
    "    for t in range(TEST_STEP_LIMIT):\n",
    "        actions, prev_coll = [], test_env.collisions\n",
    "        for i in range(NUM_AGENTS):\n",
    "            # greedy (no epsilon) action\n",
    "            actions.append(agents[i].select_action(states[i]))\n",
    "        next_states, _ = test_env.step(actions)\n",
    "\n",
    "        # detect any collision in this test step\n",
    "        if test_env.collisions > prev_coll:\n",
    "            collided = True\n",
    "        # detect dropoff completion (A→B→A cycle)\n",
    "        if not collided and any(\n",
    "                states[i][2] == 1 and next_states[i][2] == 0\n",
    "                for i in range(NUM_AGENTS)\n",
    "        ):\n",
    "            successes += 1\n",
    "            break\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "success_rate = successes / TEST_EPISODES * 100\n",
    "\n",
    "# 2) Training collisions (head-on) already tracked in `total_collisions`\n",
    "train_collisions = total_collisions\n",
    "\n",
    "print(f\"Test success rate (≤{TEST_STEP_LIMIT} steps): {success_rate:.1f}%\")\n",
    "print(f\"Head-on collisions during training: {train_collisions}\")\n",
    "\n",
    "# 3) Compute Performance Points per Table 2\n",
    "perf_points = 0\n",
    "if success_rate > 95 and train_collisions < 500:\n",
    "    perf_points = 2\n",
    "elif success_rate > 85 and train_collisions < 1000:\n",
    "    perf_points = 1\n",
    "\n",
    "print(f\"Performance points earned: {perf_points} / 2 per metric, 4 max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate (≤ 20 steps, collision-free): 100.0%\n",
      "Total head-on collisions over 1000 tests: 244\n"
     ]
    }
   ],
   "source": [
    "# In[ ]: Performance evaluation (Table 2 measurement)\n",
    "\n",
    "# Use the same environment configuration as training (toggle off_job_training if needed)\n",
    "test_env = MultiAgentEnv(off_job_training=False)\n",
    "\n",
    "successes = 0\n",
    "total_test_collisions = 0\n",
    "\n",
    "for _ in range(TEST_EPISODES):\n",
    "    states = test_env.reset()\n",
    "    prev_coll = 0\n",
    "\n",
    "    # Run up to TEST_STEP_LIMIT steps to attempt one A→B→A round-trip\n",
    "    for t in range(TEST_STEP_LIMIT):\n",
    "        # Select each agent’s greedy action (no exploration)\n",
    "        actions = [agents[i].select_action(states[i]) for i in range(NUM_AGENTS)]\n",
    "        next_states, coll = test_env.step(actions)\n",
    "\n",
    "        # Count only new head-on collisions in this step\n",
    "        total_test_collisions += (coll - prev_coll)\n",
    "        prev_coll = coll\n",
    "\n",
    "        # Detect successful drop-off: carrying→not carrying\n",
    "        if any(states[i][2] == 1 and next_states[i][2] == 0\n",
    "               for i in range(NUM_AGENTS)):\n",
    "            successes += 1\n",
    "            break\n",
    "\n",
    "        states = next_states\n",
    "\n",
    "# Compute metrics\n",
    "success_rate = successes / TEST_EPISODES * 100\n",
    "print(f\"Success rate (≤ {TEST_STEP_LIMIT} steps, collision-free): {success_rate:.1f}%\")\n",
    "print(f\"Total head-on collisions over {TEST_EPISODES} tests: {total_test_collisions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6]: Final performance check\n",
    "\n",
    "assert rate >= FINAL_SUCCESS_RATE, \\\n",
    "       f\"Final rate {rate:.2f} below target {FINAL_SUCCESS_RATE:.2f}\"\n",
    "print(\"Meets final delivery requirement.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9fcf72cb52c3da5a391be76d80cc572260da94cda47ff688c923f8cfb7406a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
