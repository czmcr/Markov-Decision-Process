{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Set up the environment\n",
    "The first task is to create a grid world where agents can move between two locations A and B. Here's a simple way to implement the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Environment settings\n",
    "grid_size = 5  # 5x5 grid\n",
    "A = (0, 0)  # Location A\n",
    "B = (4, 4)  # Location B\n",
    "actions = ['up', 'down', 'left', 'right']  # Possible actions\n",
    "\n",
    "# Grid initialization\n",
    "grid = np.zeros((grid_size, grid_size))  # Create a grid of size 5x5\n",
    "\n",
    "# Define agent class\n",
    "class Agent:\n",
    "    def __init__(self, start_position):\n",
    "        self.position = start_position  # Agent's starting position\n",
    "        self.carrying_item = False  # Whether the agent is carrying an item\n",
    "\n",
    "    def move(self, action):\n",
    "        \"\"\" Move the agent according to the action \"\"\"\n",
    "        x, y = self.position\n",
    "        if action == 'up' and x > 0:\n",
    "            self.position = (x - 1, y)\n",
    "        elif action == 'down' and x < grid_size - 1:\n",
    "            self.position = (x + 1, y)\n",
    "        elif action == 'left' and y > 0:\n",
    "            self.position = (x, y - 1)\n",
    "        elif action == 'right' and y < grid_size - 1:\n",
    "            self.position = (x, y + 1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Set up Q-learning (Tabular Q-learning)\n",
    "In Q-learning, the agent learns a policy by updating a Q-table. We'll use the Q-table to store the action-value function. Here's how to implement Q-learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration factor\n",
    "        self.q_table = {}  # Q-table to store state-action values\n",
    "\n",
    "    def get_state(self, agent):\n",
    "        \"\"\"Get the state of the agent\"\"\"\n",
    "        return agent.position + (agent.carrying_item,)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action based on epsilon-greedy policy\"\"\"\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(actions)  # Explore: choose random action\n",
    "        else:\n",
    "            # Exploit: choose the best action based on the Q-table\n",
    "            if state not in self.q_table:\n",
    "                self.q_table[state] = {a: 0 for a in actions}  # Initialize Q-values for new state\n",
    "            return max(self.q_table[state], key=self.q_table[state].get)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the Q-table based on the action taken\"\"\"\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = {a: 0 for a in actions}\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = {a: 0 for a in actions}\n",
    "\n",
    "        best_next_action = max(self.q_table[next_state], key=self.q_table[next_state].get)\n",
    "        # Q-value update formula\n",
    "        self.q_table[state][action] = self.q_table[state][action] + self.alpha * (\n",
    "            reward + self.gamma * self.q_table[next_state][best_next_action] - self.q_table[state][action])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define the task and rewards\n",
    "Now, we need to define the reward function and the criteria for avoiding head-on collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(agent, other_agents):\n",
    "    \"\"\"Define the reward based on agent's position and collision status\"\"\"\n",
    "    # Check for collisions\n",
    "    for other_agent in other_agents:\n",
    "        if agent.position == other_agent.position:\n",
    "            if (agent.carrying_item and not other_agent.carrying_item) or (not agent.carrying_item and other_agent.carrying_item):\n",
    "                return -10  # Penalty for collision (head-on)\n",
    "    \n",
    "    # Reward for delivering item from A to B or B to A\n",
    "    if agent.position == A and agent.carrying_item:\n",
    "        agent.carrying_item = False\n",
    "        return 10  # Reward for successful delivery\n",
    "    elif agent.position == B and not agent.carrying_item:\n",
    "        agent.carrying_item = True\n",
    "        return 0  # No reward for arriving at B without an item\n",
    "    \n",
    "    return -1  # Small penalty for unnecessary movements\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Training the agents\n",
    "Now, we can set up the environment, initialize the agents, and train them using Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "agents = [Agent(A) for _ in range(4)]\n",
    "q_learning_agents = [QLearningAgent() for _ in range(4)]\n",
    "\n",
    "# Training loop\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    for agent, q_agent in zip(agents, q_learning_agents):\n",
    "        state = q_agent.get_state(agent)\n",
    "        action = q_agent.choose_action(state)\n",
    "        \n",
    "        # Move the agent\n",
    "        agent.move(action)\n",
    "        \n",
    "        # Get reward and next state\n",
    "        reward = get_reward(agent, agents)\n",
    "        next_state = q_agent.get_state(agent)\n",
    "        \n",
    "        # Update Q-table\n",
    "        q_agent.update_q_table(state, action, reward, next_state)\n",
    "\n",
    "    # Optionally print progress\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}/{episodes} complete\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Testing the learned policy\n",
    "After training, the agents should be able to move from A to B and back, coordinating to avoid collisions. We can test the learned behavior as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agents():\n",
    "    for agent in agents:\n",
    "        print(f\"Agent started at {agent.position}\")\n",
    "        # Test the agent's actions\n",
    "        state = q_learning_agents[0].get_state(agent)\n",
    "        action = q_learning_agents[0].choose_action(state)\n",
    "        agent.move(action)\n",
    "        print(f\"Agent moved to {agent.position}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9fcf72cb52c3da5a391be76d80cc572260da94cda47ff688c923f8cfb7406a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
