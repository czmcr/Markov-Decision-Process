{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import random\n",
    "import pickle\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "GRID_SIZE = 5\n",
    "NUM_AGENTS = 4\n",
    "A_LOCATION = (1, 1)\n",
    "B_LOCATION = (4, 4)\n",
    "\n",
    "# Actions: 0=North, 1=South, 2=West, 3=East\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Represents an agent in the grid world with a simple Q-table.\n",
    "    Each agent learns to move from A to B and back while avoiding head-on collisions.\n",
    "    Includes use of:\n",
    "      - State of neighboring cells (only considers agents of opposite type)\n",
    "      - Central clock (update schedule is round-robin)\n",
    "      - Off-the-job training (start configs defined below)\n",
    "    \"\"\"\n",
    "    def __init__(self, idx, shared_q=None):\n",
    "        self.idx = idx\n",
    "        self.reset()\n",
    "        # Use shared Q-table if provided, otherwise create individual Q-table\n",
    "        self.q_table = shared_q if shared_q is not None else {}\n",
    "        self.epsilon = 0.2\n",
    "        self.alpha = 1e-3\n",
    "        self.gamma = 0.99\n",
    "        self.rewards_log = []\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Sets initial position and carrying status for off-the-job training.\"\"\"\n",
    "        self.pos = A_LOCATION if self.idx % 2 == 0 else B_LOCATION\n",
    "        self.carrying = self.pos == A_LOCATION\n",
    "\n",
    "    def reset_at_b(self):\n",
    "        \"\"\"Reset agent to start at B for performance evaluation.\"\"\"\n",
    "        self.pos = B_LOCATION\n",
    "        self.carrying = False  # Not carrying when starting at B\n",
    "\n",
    "    def get_neighbors_state(self, grid, agents):\n",
    "        \"\"\"\n",
    "        Returns a binary string where 1 means an adjacent cell has an agent\n",
    "        of opposite carrying state (e.g., one is carrying and the other is not).\n",
    "        \"\"\"\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # N, S, W, E only\n",
    "        bits = ''\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = self.pos[0] + dx, self.pos[1] + dy\n",
    "            if 0 <= nx < GRID_SIZE and 0 <= ny < GRID_SIZE:\n",
    "                agent_id = grid[nx][ny]\n",
    "                if agent_id != -1:\n",
    "                    other_agent = agents[agent_id]\n",
    "                    if other_agent.carrying != self.carrying:\n",
    "                        bits += '1'\n",
    "                        continue\n",
    "            bits += '0'\n",
    "        return bits\n",
    "\n",
    "    def get_state(self, grid, agents):\n",
    "        neighbors = self.get_neighbors_state(grid, agents)\n",
    "        return (self.pos[0], self.pos[1], int(self.carrying), neighbors)\n",
    "\n",
    "    def choose_action(self, grid, agents, eval_mode=False):\n",
    "        state = self.get_state(grid, agents)\n",
    "        if not eval_mode and random.random() < self.epsilon or state not in self.q_table:\n",
    "            return random.randint(0, 3)\n",
    "        return int(np.argmax(self.q_table[state]))\n",
    "\n",
    "    def update_q(self, prev_state, action, reward, next_state):\n",
    "        if prev_state not in self.q_table:\n",
    "            self.q_table[prev_state] = [0.0] * 4\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = [0.0] * 4\n",
    "        max_next = max(self.q_table[next_state])\n",
    "        self.q_table[prev_state][action] += self.alpha * (reward + self.gamma * max_next - self.q_table[prev_state][action])\n",
    "\n",
    "    def move(self, action):\n",
    "        dx, dy = actions[action]\n",
    "        new_x = min(max(self.pos[0] + dx, 0), GRID_SIZE - 1)\n",
    "        new_y = min(max(self.pos[1] + dy, 0), GRID_SIZE - 1)\n",
    "        self.pos = (new_x, new_y)\n",
    "\n",
    "    def update_carrying_status(self):\n",
    "        \"\"\"Updates carrying status based on current position\"\"\"\n",
    "        # When at A, pickup supply (become carrying)\n",
    "        if self.pos == A_LOCATION and not self.carrying:\n",
    "            self.carrying = True\n",
    "            return 1  # Reward for pickup\n",
    "        # When at B, deliver supply (become not carrying)\n",
    "        elif self.pos == B_LOCATION and self.carrying:\n",
    "            self.carrying = False\n",
    "            return 1  # Reward for delivery\n",
    "        return 0  # No reward if no pickup/delivery happened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a shared Q-table for all agents\n",
    "shared_q_table = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agents with shared Q-table\n",
    "agents = [Agent(i, shared_q=shared_q_table) for i in range(NUM_AGENTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training pipeline constraints\n",
    "step_budget = 1_500_000\n",
    "collision_budget = 4000\n",
    "walltime_budget = 600  # 10 minutes in seconds\n",
    "start_time = time.time()\n",
    "total_steps = 0\n",
    "collision_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_on_collision(agent, other):\n",
    "    return agent.pos == other.pos and agent.carrying != other.carrying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with budgets\n",
    "print(\"Training agents with step/collision/time budget...\")\n",
    "episode = 0\n",
    "while total_steps < step_budget and collision_count < collision_budget and (time.time() - start_time) < walltime_budget:\n",
    "    grid = [[-1 for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)]\n",
    "    for agent in agents:\n",
    "        agent.reset()\n",
    "        grid[agent.pos[0]][agent.pos[1]] = agent.idx\n",
    "\n",
    "    for step in range(20):\n",
    "        for agent in agents:\n",
    "            total_steps += 1\n",
    "            state = agent.get_state(grid, agents)\n",
    "            action = agent.choose_action(grid, agents)\n",
    "            dx, dy = actions[action]\n",
    "            proposed_x = min(max(agent.pos[0] + dx, 0), GRID_SIZE - 1)\n",
    "            proposed_y = min(max(agent.pos[1] + dy, 0), GRID_SIZE - 1)\n",
    "\n",
    "            # Allow move only if the target cell is empty OR occupied by agents going in the same direction\n",
    "            occupying_agent_id = grid[proposed_x][proposed_y]\n",
    "            if occupying_agent_id != -1:\n",
    "                occupying_agent = agents[occupying_agent_id]\n",
    "                if occupying_agent.carrying != agent.carrying:\n",
    "                    reward = -1  # discourage illegal move into conflicting traffic\n",
    "                    agent.rewards_log.append(reward)\n",
    "                    continue\n",
    "\n",
    "            # Clear the agent's position in the grid\n",
    "            grid[agent.pos[0]][agent.pos[1]] = -1\n",
    "            old_pos = agent.pos\n",
    "            agent.move(action)\n",
    "\n",
    "            # Initialize reward\n",
    "            reward = 0\n",
    "            \n",
    "            # Add reward for completing pickup or delivery\n",
    "            reward += agent.update_carrying_status()\n",
    "            \n",
    "            # Check for collisions\n",
    "            for other in agents:\n",
    "                if other is not agent and head_on_collision(agent, other):\n",
    "                    reward = -10\n",
    "                    collision_count += 1\n",
    "\n",
    "            agent.rewards_log.append(reward)\n",
    "            next_state = agent.get_state(grid, agents)\n",
    "            agent.update_q(state, action, reward, next_state)\n",
    "            \n",
    "            # Update the agent's position in the grid\n",
    "            grid[agent.pos[0]][agent.pos[1]] = agent.idx\n",
    "\n",
    "    episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training completed: Episodes={episode}, Steps={total_steps}, Collisions={collision_count}, Time Elapsed={time.time() - start_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained Q-tables for future use\n",
    "with open(\"q_tables.pkl\", \"wb\") as f:\n",
    "    pickle.dump([shared_q_table], f)  # Save only the shared Q-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation Function\n",
    "def evaluate_performance(num_trials=100, max_steps=25):\n",
    "    \"\"\"\n",
    "    Evaluates agent performance starting at B.\n",
    "    Success criteria: Complete delivery (B→A→B) within max_steps steps without collisions.\n",
    "    Returns success rate.\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating final performance over {num_trials} trials ({max_steps} steps max)...\")\n",
    "    \n",
    "    successful_trials = 0\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        # Reset environment\n",
    "        grid = [[-1 for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)]\n",
    "        \n",
    "        # Reset all agents to start at B\n",
    "        for agent in agents:\n",
    "            agent.reset_at_b()\n",
    "            grid[agent.pos[0]][agent.pos[1]] = agent.idx\n",
    "        \n",
    "        # Track if this trial had any collisions\n",
    "        collision_occurred = False\n",
    "        # Track if agents completed a delivery in this trial\n",
    "        delivery_completed = [False] * NUM_AGENTS\n",
    "        \n",
    "        # Run the trial for max_steps steps\n",
    "        for step in range(max_steps):\n",
    "            for agent_idx, agent in enumerate(agents):\n",
    "                # Get state and choose action (no exploration during evaluation)\n",
    "                state = agent.get_state(grid, agents)\n",
    "                action = agent.choose_action(grid, agents, eval_mode=True)\n",
    "                \n",
    "                # Clear agent's current position in grid\n",
    "                grid[agent.pos[0]][agent.pos[1]] = -1\n",
    "                \n",
    "                # Execute move\n",
    "                agent.move(action)\n",
    "                \n",
    "                # Check for delivery completion: B→A→B cycle\n",
    "                if agent.pos == A_LOCATION and not agent.carrying:\n",
    "                    agent.carrying = True  # Pick up at A\n",
    "                elif agent.pos == B_LOCATION and agent.carrying:\n",
    "                    agent.carrying = False  # Deliver at B\n",
    "                    delivery_completed[agent_idx] = True\n",
    "                \n",
    "                # Check for collisions\n",
    "                for other in agents:\n",
    "                    if other is not agent and head_on_collision(agent, other):\n",
    "                        collision_occurred = True\n",
    "                \n",
    "                # Update grid with new position\n",
    "                grid[agent.pos[0]][agent.pos[1]] = agent.idx\n",
    "            \n",
    "            # If all agents completed delivery or collision occurred, end trial\n",
    "            if all(delivery_completed) or collision_occurred:\n",
    "                break\n",
    "        \n",
    "        # Trial is successful if at least one agent completed delivery and no collisions\n",
    "        if any(delivery_completed) and not collision_occurred:\n",
    "            successful_trials += 1\n",
    "    \n",
    "    success_rate = successful_trials / num_trials * 100\n",
    "    print(f\"Performance results: {successful_trials}/{num_trials} successful trials ({success_rate:.2f}%)\")\n",
    "    \n",
    "    # Check if performance meets requirement (75% success rate)\n",
    "    if success_rate >= 75:\n",
    "        print(\"Performance requirement met (≥75% success rate)\")\n",
    "    else:\n",
    "        print(\"Performance requirement not met (<75% success rate)\")\n",
    "    \n",
    "    return success_rate\n",
    "\n",
    "# Run evaluation after training\n",
    "evaluation_result = evaluate_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
