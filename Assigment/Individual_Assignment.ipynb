{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import random\n","import time\n","\n","# --- CONSTANTS ---\n","GRID_SIZE = 5\n","NUM_AGENTS = 4\n","ACTIONS = ['N', 'S', 'W', 'E']\n","ACTION_TO_DELTA = {'N': (-1, 0), 'S': (1, 0), 'W': (0, -1), 'E': (0, 1)}"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# --- HYPERPARAMETERS ---\n","ALPHA = 0.1        # Learning rate\n","GAMMA = 0.9        # Discount factor\n","EPSILON = 0.1      # Exploration rate\n","MAX_STEPS = 1500000\n","MAX_COLLISIONS = 4000\n","MAX_TRAINING_TIME = 600  # 10 mins"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# --- ENVIRONMENT SETUP ---\n","class Agent:\n","    def __init__(self, idx, start_pos, carrying=False):\n","        self.id = idx\n","        self.pos = start_pos\n","        self.carrying = carrying\n","\n","    def reset(self, start_pos, carrying=False):\n","        self.pos = start_pos\n","        self.carrying = carrying\n","\n","class Environment:\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        # Random positions for A and B\n","        self.loc_A = (np.random.randint(0, GRID_SIZE), np.random.randint(0, GRID_SIZE))\n","        while True:\n","            self.loc_B = (np.random.randint(0, GRID_SIZE), np.random.randint(0, GRID_SIZE))\n","            if self.loc_B != self.loc_A:\n","                break\n","\n","        # Place 4 agents at A or B randomly\n","        self.agents = []\n","        for i in range(NUM_AGENTS):\n","            start = self.loc_A if np.random.rand() < 0.5 else self.loc_B\n","            carrying = (start == self.loc_A)\n","            self.agents.append(Agent(i, start, carrying))\n","\n","    def step(self, actions):\n","        rewards = [0] * NUM_AGENTS\n","        collisions = 0\n","        next_positions = []\n","\n","        # Step 1: Compute next positions\n","        for i, agent in enumerate(self.agents):\n","            dx, dy = ACTION_TO_DELTA[actions[i]]\n","            nx = max(0, min(GRID_SIZE - 1, agent.pos[0] + dx))\n","            ny = max(0, min(GRID_SIZE - 1, agent.pos[1] + dy))\n","            next_positions.append((nx, ny))\n","\n","        # Step 2: Check for head-on collisions\n","        for i in range(NUM_AGENTS):\n","            for j in range(i+1, NUM_AGENTS):\n","                if next_positions[i] == self.agents[j].pos and next_positions[j] == self.agents[i].pos:\n","                    # Head-on collision\n","                    collisions += 1\n","                    rewards[i] -= 10\n","                    rewards[j] -= 10\n","                    next_positions[i] = self.agents[i].pos  # revert\n","                    next_positions[j] = self.agents[j].pos\n","\n","        # Step 3: Update agents\n","        for i, agent in enumerate(self.agents):\n","            agent.pos = next_positions[i]\n","\n","            # Check for pickup/drop-off\n","            if not agent.carrying and agent.pos == self.loc_A:\n","                agent.carrying = True\n","            elif agent.carrying and agent.pos == self.loc_B:\n","                agent.carrying = False\n","                rewards[i] += 10  # successful delivery\n","\n","            # Small penalty per move\n","            rewards[i] -= 1\n","\n","        return rewards, collisions"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# --- STATE ENCODING ---\n","def encode_state(agent, loc_A, loc_B):\n","    # (x, y, Ax, Ay, Bx, By, carrying)\n","    return (\n","        agent.pos[0], agent.pos[1],\n","        loc_A[0], loc_A[1],\n","        loc_B[0], loc_B[1],\n","        int(agent.carrying)\n","    )"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# --- Q-LEARNING ---\n","Q = {}  # Q-table: key=(state, action), value=Q-value\n","\n","def get_Q(state, action):\n","    return Q.get((state, action), 0.0)\n","\n","def select_action(state):\n","    if np.random.rand() < EPSILON:\n","        return random.choice(ACTIONS)\n","    qs = [get_Q(state, a) for a in ACTIONS]\n","    return ACTIONS[np.argmax(qs)]\n","\n","def update_Q(state, action, reward, next_state):\n","    max_next = max([get_Q(next_state, a) for a in ACTIONS])\n","    old_value = get_Q(state, action)\n","    Q[(state, action)] = old_value + ALPHA * (reward + GAMMA * max_next - old_value)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training complete. Total steps: 25056, collisions: 4000, time: 0.97s\n"]}],"source":["# --- TRAINING LOOP ---\n","env = Environment()\n","start_time = time.time()\n","total_steps = 0\n","total_collisions = 0\n","\n","while total_steps < MAX_STEPS and time.time() - start_time < MAX_TRAINING_TIME and total_collisions < MAX_COLLISIONS:\n","    for agent_id in range(NUM_AGENTS):  # Central Clock: Round Robin\n","        agent = env.agents[agent_id]\n","        state = encode_state(agent, env.loc_A, env.loc_B)\n","        action = select_action(state)\n","\n","        # One agent acts, others wait\n","        actions = ['X'] * NUM_AGENTS\n","        actions[agent_id] = action\n","        for j in range(NUM_AGENTS):\n","            if actions[j] == 'X':\n","                actions[j] = select_action(encode_state(env.agents[j], env.loc_A, env.loc_B))\n","\n","        rewards, collisions = env.step(actions)\n","        total_collisions += collisions\n","\n","        next_state = encode_state(agent, env.loc_A, env.loc_B)\n","        update_Q(state, action, rewards[agent_id], next_state)\n","        total_steps += 1\n","\n","    # Occasionally reset environment for exploration\n","    if total_steps % 1000 == 0:\n","        env.reset()\n","\n","print(f\"Training complete. Total steps: {total_steps}, collisions: {total_collisions}, time: {time.time() - start_time:.2f}s\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Q"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"}}},"nbformat":4,"nbformat_minor":2}
